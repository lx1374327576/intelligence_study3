    代码地址 https://github.com/lx1374327576/intelligence_study3
    训练过程可以在branch master commit上查看
# task1

abstract:
    用cnn(conv->relu->pool->fc->relu->fc)在mnist上实现了0.92的准确率

    1.代码实现:
        根据之前学习的经验，采用了core->model->solver->main的架构，先实现每个单元层的正相
        传播和反向传播，然后写模型，这里采用了最常见的cnn模型。然后写训练框架，都不是很复杂，
        主要时间还是花在了调试上。
    2.碰到的问题：
        当loss不下降的时候，怎么判断哪里除了问题？
        首先先排除每一层可能写错的情况，我想了想觉得一般写错会发生在式子推错/代码手误上，所以
        我把每层跑出来的结果看了一下，结果发现并没有什么问题。每层的参数也没有发生梯度消失或
        爆炸的问题。然后发现numpy数组有一个深浅复制的问题，我觉得可能会影响计算，所以将所有
        复制改成了深复制。接下来我把目光放在了learning_rate=1e-2上，一般情况下learning_rate
        不够大也会发生loss不下降，所以抱着试试的心态将learning_rate改成了1，果然发生了梯度
        爆炸，最后定下了learning_rate和dacay, 解决了这个问题。
        
        numpy数组维度报错？
        看了下之后发现是卷积层之后没有flatten(), 于是在fc的时候reshape了一下
        
        acc徘徊在0.8左右？
        刚开始由于效率的原因没有加relu层，采用的conv->fc->fc的架构，在一定时间以后徘徊在0.8
        左右。加上了relu以后，cnn的收敛速度下降了好多，精度大概上升了到了0.83。我觉得这不是
        问题的所在，在仔细查看代码以后，我发现之前由于性能的原因，采取训练100个数据，验证100个
        数据。于是我把100个改成了500个，acc就到了0.92。
        
        训练速度不够快？
        我看了看每一块跑的时间，发现时间主要消耗在了conv上，由于是用循环写的，慢也没办法，这个
        问题还没有解决

# task2

abstract:
    用tensorflow框架，cnn(conv_relu->bn->pool->conv_relu->bn->pool->fc_relu->fc)在
    cifar10数据集上实现了0.798的准确率
    
    1.代码实现:
        使用的tensorflow框架，代码实现上没有什么问题
        
    2.碰到的问题
        tensorflow接受的图片数组的格式和读到的不太一样，根据提示调成一样的就行了
        
        如何提高准确率?
        其实这是一个怎么平衡时间的问题，由于我的电脑训练一次要4个小时，所以每一次调整参数都比较
        宝贵。刚开始没有加batch_normalization，每层参数每次波动比较大，准确率只有0.22，于是
        加上了bn。在后面的训练和实时观察中发现，由于训练速度不变，导致loss过早的收敛，就加入
        了sgd函数。最后觉得一层fc不太够然后加到了两层。
       
reference: 两个数据集读取来自cs231n
            